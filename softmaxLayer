import random
import numpy as np
import activationFunctions
import layer

class softmaxLayer(layer):
    def activation():
        pass
    
    def activationDerivative():
        pass
    
    weights = []
    bias = []
    learningRate = 0.1
    size = 0
    inputSize = 0
    
    actFunc = activation
    actFuncDerivative = activationDerivative
    
    def __init__(self, size, inputSize, activation=activationFunctions.tanh, activationD=activationFunctions.tanhD) -> None:
        self.size = size
        self.inputSize = inputSize
        
    def reinit(self) -> None:
        pass
    
    #takes input as matrix, for use inside the network
    def forward(self, input):
        output = activationFunctions.softmax(input)
        return output
    
    def backPropagation(self, input, output, error):
        pass
    
    def findError(self, prevError):
        return prevError
    